{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <center><font color='red'> TME 1 - Régression linéaire et descente de gradient </font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne pas oublier d'executer dans le shell avant de lancer python :\n",
    "# source /users/Enseignants/piwowarski/venv/amal/3.7/bin/activate\n",
    "\n",
    "import timeit\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "# from datamaestro import prepare_dataset \n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    \"\"\"Very simplified context object\"\"\"\n",
    "    def __init__(self):\n",
    "        self._saved_tensors = ()\n",
    "    def save_for_backward(self, *args):\n",
    "        self._saved_tensors = args\n",
    "    @property\n",
    "    def saved_tensors(self):\n",
    "        return self._saved_tensors\n",
    "\n",
    "\n",
    "# class MaFonction(Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx,input):\n",
    "#         ctx.save_for_backward(input)\n",
    "#         return None\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx,grad_output):\n",
    "#         input = ctx.saved_tensors\n",
    "#         return None\n",
    "\n",
    "\n",
    "## Exemple d'implementation de fonction a 2 entrÃ©es\n",
    "class MaFonction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,x,w):\n",
    "        ## Calcul la sortie du module\n",
    "        ctx.save_for_backward(x,w)\n",
    "        return torch.mm(x, w.T)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, forward_output):\n",
    "        ## Calcul du gradient du module par rapport a chaque groupe d'entrÃ©es\n",
    "        x,w = ctx.saved_tensors\n",
    "        return torch.mm( forward_output, w), torch.mm( forward_output.T , x )\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(yhat, y):\n",
    "        return ((yhat-y)**2).mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def sgb(ctx, grad_output, eps):\n",
    "        x,w = ctx.saved_tensors\n",
    "        w = w - eps * grad_output[1]\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> 1. Préliminaire - Données random </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10,5,requires_grad=True,dtype=torch.float64)\n",
    "w = torch.randn(1,5,requires_grad=True,dtype=torch.float64)\n",
    "\n",
    "## Pour utiliser la fonction \n",
    "mafonction = MaFonction()\n",
    "ctx = Context()\n",
    "forward = mafonction.forward(ctx,x,w)\n",
    "backward = mafonction.backward(ctx, forward)\n",
    "\n",
    "## Pour tester le gradient \n",
    "mafonction_check = MaFonction.apply\n",
    "torch.autograd.gradcheck(mafonction_check,(x,w))\n",
    "\n",
    "# ## Pour telecharger le dataset Boston\n",
    "# ds=prepare_dataset(\"edu.uci.boston\")\n",
    "# fields, data =ds.files.data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> 2. Application des descentes de gradient sur les données Boston </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([506, 13]) torch.Size([506])\n"
     ]
    }
   ],
   "source": [
    "X, y = load_boston(return_X_y=True) # (506, 13)  (506,)\n",
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgbBoston(data, labels, pc_train, version, eps=1e-5):\n",
    "    size_split = int(pc_train*len(data))\n",
    "    X_train, X_test = data[:size_split], data[size_split:]\n",
    "    y_train, y_test = labels[:size_split], labels[size_split:]\n",
    "    w = torch.randn(1, 13, requires_grad=True, dtype=torch.float64)\n",
    "    mafonction = MaFonction()\n",
    "    ctx = Context()\n",
    "    mse_train = []\n",
    "    mse_test = []\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    tic=timeit.default_timer()\n",
    "    \n",
    "    if version == \"batch\":\n",
    "        for i in range(200):\n",
    "            for j in range(50):\n",
    "                forward = mafonction.forward(ctx, X_train, w)\n",
    "                backward = mafonction.backward(ctx, forward)\n",
    "                mse = mafonction.mse(forward, y_train)\n",
    "                #mse_train.append(mse)\n",
    "                writer.add_scalar('MSE/batch/train', mse, j)\n",
    "                w = mafonction.sgb(ctx, backward, eps)\n",
    "            forward = mafonction.forward(ctx, X_test, w)\n",
    "            mse = mafonction.mse(forward, y_test)\n",
    "            #mse_test.append(mse)\n",
    "            writer.add_scalar('MSE/batch/test', mse, i)\n",
    "            \n",
    "    elif version == \"stochastique\":\n",
    "        for i in range(200):\n",
    "            for j in range(50):\n",
    "                rand = np.random.randint(0,len(X_train))\n",
    "                forward = mafonction.forward(ctx, X_train[rand].reshape((1,13)), w)\n",
    "                backward = mafonction.backward(ctx, forward)\n",
    "                mse = mafonction.mse(forward, y_train[rand])\n",
    "                #mse_train.append(mse)\n",
    "                writer.add_scalar('MSE/stoch/train', mse, j)\n",
    "                w = mafonction.sgb(ctx, backward, eps)\n",
    "            forward = mafonction.forward(ctx, X_test, w)\n",
    "            mse = mafonction.mse(forward, y_test)\n",
    "            #mse_test.append(mse)\n",
    "            writer.add_scalar('MSE/stoch/test', mse, i)\n",
    "            \n",
    "    elif version == \"minibatch\":\n",
    "        for i in range(200):\n",
    "            for j in range(0, len(X_train)-10, 10):\n",
    "                forward = mafonction.forward(ctx, X_train[j:j+10], w)\n",
    "                backward = mafonction.backward(ctx, forward)\n",
    "                mse = mafonction.mse(forward, y_train[j:j+10])\n",
    "                #mse_train.append(mse)\n",
    "                writer.add_scalar('MSE/minib/train', mse, j)\n",
    "                w = mafonction.sgb(ctx, backward, eps)\n",
    "            forward = mafonction.forward(ctx, X_test, w)\n",
    "            mse = mafonction.mse(forward, y_test)\n",
    "            #mse_test.append(mse)\n",
    "            writer.add_scalar('MSE/minib/test', mse, i)\n",
    "            \n",
    "    else:\n",
    "        print(\"Version inexistante, essayez : minibatch | batch | stochastique\")\n",
    "        \n",
    "    toc=timeit.default_timer()\n",
    "    \n",
    "    print(\"Variant : \", version)\n",
    "    print(\"Learning rate : \", eps)\n",
    "    print(\"TIME : \", toc-tic)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant :  batch\n",
      "Learning rate :  1e-10\n",
      "TIME :  19.63852430000003\n",
      "Variant :  stochastique\n",
      "Learning rate :  1e-10\n",
      "TIME :  5.9461709999999925\n",
      "Variant :  minibatch\n",
      "Learning rate :  1e-10\n",
      "TIME :  4.064470300000039\n"
     ]
    }
   ],
   "source": [
    "versions = [\"batch\", \"stochastique\", \"minibatch\"]\n",
    "\n",
    "# Décommentez le bloc suivant pour calculer la descente de gradient des 3 versions\n",
    "# for v in versions:\n",
    "#     sgbBoston(X, y, 0.8, v, 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"> Pour pouvoir afficher les résultats : \n",
    "    <ul>\n",
    "        <li> Ouvrir l'anaconda prompt </li>\n",
    "        <li> Se placer dans le dossier contenant le sous-dossier \"runs\" </li>\n",
    "        <li> Taper la commande : tensorboard --logdir=runs </li>\n",
    "        <li> Taper sur google le lien localhost affiché </li>\n",
    "    </ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MSE](mse_graphes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"> \n",
    "Voici les résultats qu'on a obtenu ci-dessus avec les différentes variantes de la descente de gradient et avec un learning rate de 1e-10. <br>\n",
    "    \n",
    "Dans un premier temps, on remarque que la vitesse de convergence de la variante batch est 4 fois plus lente que les deux autres variantes (20s vs 5s), ce qui n'est pas surprenant vu qu'on prend en compte l'ensemble de nos données pour l'apprentissage cependant cette variante comporte divers avantages comme par exemple un taux d'apprentissage qui ne décroit pas, la garantie de converger à un minimum global si la fonction loss est convexe puis l'estimation du gradient se retrouve non biaisé. <br>\n",
    "    \n",
    "Concernant la variante mini-batch, on observe sur la courbe de train plusieurs oscillations représentant le bruit ajouté puisque la taille du batch est inférieure à celle de l'ensemble du train, à cause de ce bruit, il n'est pas possible de converger sans faire décroitre le learning rate. <br>\n",
    "    \n",
    "Concernant la variante stochastique, elle présente encore plus de bruit que la variante minibatch et également un temps d'exécution plus long. <br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
