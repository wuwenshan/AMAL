{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <center><font color='red'> TME 2 - Graphe de calcul, autograd et modules </font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    \"\"\"Very simplified context object\"\"\"\n",
    "    def __init__(self):\n",
    "        self._saved_tensors = ()\n",
    "    def save_for_backward(self, *args):\n",
    "        self._saved_tensors = args\n",
    "    @property\n",
    "    def saved_tensors(self):\n",
    "        return self._saved_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chargement des données Boston**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True) # (506, 13)  (506,)\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "X.requires_grad = True\n",
    "\n",
    "y = torch.from_numpy(y)\n",
    "y.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> 1. Différenciation automatique : autograd </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exemple d'implementation de fonction a 2 entrÃ©es\n",
    "class MaFonctionV1(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, w):\n",
    "        ## Calcul la sortie du module\n",
    "        ctx.save_for_backward(x,w)\n",
    "        return torch.mm(x, w.T)\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(yhat, y):\n",
    "        return ((yhat-y)**2).mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def sgb(ctx, eps):\n",
    "        x,w = ctx.saved_tensors\n",
    "        with torch.no_grad():\n",
    "            w -= eps * w.grad\n",
    "            w.grad.zero_()\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf = MaFonctionV1()\n",
    "ctx = Context()\n",
    "\n",
    "# Split en train/test sans shuffle\n",
    "size_split = int(0.8*len(X))\n",
    "X_train, X_test = X[:size_split], X[size_split:]\n",
    "y_train, y_test = y[:size_split], y[size_split:]\n",
    "\n",
    "# Initialisation des poids façon random\n",
    "w = torch.rand((1,X_train.shape[1]), requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for i in range(200):\n",
    "    for j in range(0, len(X_train)-10, 10):\n",
    "        forward = maf.forward(ctx, X_train[j:j+10], w)\n",
    "        l = maf.mse(forward, y_train[j:j+10])\n",
    "        l.backward()\n",
    "        writer.add_scalar(\"Loss/Train\", l, j)\n",
    "        w = maf.sgb(ctx, 1e-9)\n",
    "    forward = maf.forward(ctx, X_test, w)\n",
    "    mse = maf.mse(forward, y_test)\n",
    "    writer.add_scalar(\"Loss/Test\", mse, i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> 2. Module </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec conteneur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exemple d'implementation de fonction a 2 entrÃ©es\n",
    "class MaFonctionV2(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def init(nx, nh, ny, eps):\n",
    "        model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(nx, nh),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(nh, ny)\n",
    "        )\n",
    "        loss = torch.nn.MSELoss()\n",
    "        optim = torch.optim.SGD(model.parameters(), lr=eps)\n",
    "        return model, loss, optim\n",
    "        \n",
    "    @staticmethod\n",
    "    def mse(loss, yhat, y):\n",
    "        return loss(yhat.T,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf = MaFonctionV2()\n",
    "ctx = Context()\n",
    "\n",
    "size_split = int(0.8*len(X))\n",
    "X_train, X_test = X[:size_split], X[size_split:]\n",
    "y_train, y_test = y[:size_split], y[size_split:]\n",
    "w = torch.rand((1,X_train.shape[1]), requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "nx = X.shape[1]\n",
    "nh = 13\n",
    "ny = len(y)\n",
    "\n",
    "model, loss, optim = maf.init(nx, nh, ny, 0.01)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for i in range(200):\n",
    "    for j in range(0, len(X_train)-10, 10):\n",
    "        forward = model(X_train[j:j+10].float())\n",
    "        l = maf.mse(loss, forward, y_train[j:j+10])\n",
    "        l.backward()\n",
    "        writer.add_scalar(\"Loss/Train\", l, j)\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    forward = model(X_test.float())\n",
    "    mse = maf.mse(loss, forward, y_test)\n",
    "    writer.add_scalar(\"Loss/Test\", mse, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sans conteneur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exemple d'implementation de fonction a 2 entrÃ©es\n",
    "class MaFonctionV3(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, nx, nh, ny):\n",
    "        super(MaFonctionV3, self).__init__()\n",
    "        self.linear = torch.nn.Linear(nx, nh)\n",
    "        self.linear2 = torch.nn.Linear(nh, ny)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = torch.nn.functional.tanh(self.linear(x))\n",
    "        return self.linear2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_split = int(0.8*len(X))\n",
    "X_train, X_test = X[:size_split], X[size_split:]\n",
    "y_train, y_test = y[:size_split], y[size_split:]\n",
    "\n",
    "nx = X.shape[1]\n",
    "nh = 13\n",
    "ny = len(y)\n",
    "\n",
    "maf = MaFonctionV3(nx, nh, ny)\n",
    "maf = maf.double()\n",
    "model = maf.cpu()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-5)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for i in range(200):\n",
    "    for j in range(0, len(X_train)-10, 10):\n",
    "        model.train()\n",
    "        pred = model(X_train[j:j+10]).double()\n",
    "        loss = criterion(pred.T.double(), y_train[j:j+10].double())\n",
    "        writer.add_scalar('Loss/train', loss, j)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    forward = model(X_test.double())\n",
    "    mse = criterion(forward.T, y_test)\n",
    "    writer.add_scalar(\"Loss/Test\", mse, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
